%\documentclass[slides]{beamer} %switch "slides" to "handout" for printing out
\documentclass[handout]{beamer}

\include{preamble}

%presentation preamble
\usetheme{progressbar}
\usecolortheme{progressbar} 
\usefonttheme{progressbar} 
\useoutertheme{progressbar}
\useinnertheme{progressbar}

\title[Lec 1]{Predictive Analytics Lecture 3}
\institute[Wharton, Statistics]{Stat 422/722\\ at The Wharton School of the University of Pennsylvania}
\date{January 31 \& February 1, 2017}

\author{Adam Kapelner}


\begin{document}

%immediately create a title page
\frame{\titlepage}


\begin{frame}\frametitle{The Coin Example from Last Class I}
\small
I want to explain the coin example from last class in the context of likelihood. Imagine you flip a coin three times and get heads, heads, tails; thus,  $y_1 = 1, y_2 = 1, y_3 = 0$. There is a true probability of heads called $\theta$. We don't know it. \\~\\

What is the probability of the data? We employ the mass / density function:


\beqn
&& \prob{Y_1 = y_1, Y_2 = y_2, Y_3 = y_3; \theta} = \prod_{i=1}^3 \prob{Y_i = y_i; \theta} = \pause  \prod_{i=1}^3 \theta^{y_i} \tothepow{1 - \theta}{1 - y_i} \\
&=& \pause \parens{ \theta^{(1)} \tothepow{1 - \theta}{1 - (1)}} \parens{\theta^{(1)} \tothepow{1 - \theta}{1 - (1)}} \parens{\theta^{(0)} \tothepow{1 - \theta}{1 - (0)}} \\
&=& \theta^2 (1 - \theta)
\eeqn

And now we can calculate the probability of seeing the data assuming $\theta$. Assume $\theta = 0.5$ then,

\beqn
\prob{Y_1 = 1, Y_2 = 1, Y_3 = 0; \theta = 0.5}  =  0.5^2 (1 - 0.5) = 0.125
\eeqn
	
\end{frame}

\begin{frame}\frametitle{The Coin Example from Last Class II}

\footnotesize
Now we ask the inverse question. If we saw this data  $y_1 = 1$, $y_2 = 1$, $y_3 = 0$, what is the most likely model, i.e. the most likely value of $\theta$. We first write down the likelihood function which it's easy because it's the same as the mass / density function


\beqn
\lik{\theta; Y_1 = 1, Y_2 = 1, Y_3 = 0} = \prob{Y_1 = 1, Y_2 = 1, Y_3 = 0; \theta} = \theta^2 (1 - \theta)
\eeqn

And now we pick the value of $\theta$ which maximizes the likelihood,

\beqn
\thetahat := \argmax_{\theta \in \Theta} \braces{\lik{\theta; x}}
\eeqn

So we need to take the derivative 

\beqn %\frac{d}{d\theta}\bracks{\lik{\theta; Y_1 = 1, Y_2 = 1, Y_3 = 0}} = \pause 
&& \frac{d}{d\theta}\bracks{\theta^2 (1 - \theta)} = \pause \frac{d}{d\theta}\bracks{\theta^2 - \theta^3} = \pause 2\theta - 3\theta^2
\eeqn

and set it equal to zero:
\beqn
0 = 2\theta - 3\theta^2 = \theta(2 - 3\theta) \Rightarrow 0 = 2-3\theta \Rightarrow \thetahatmle = \frac{2}{3}
\eeqn

i.e. the most likely model for this data is a weighted coin with probability of heads of 2/3.

\end{frame}

\section{Design}

\begin{frame}\frametitle{Dataframe Design}

We spoke a lot about featurization i.e. selecting the columns in the dataframe (these are the predictors to measure). Once we did this, we can then go out and sample observations and then measure each for their predictor values. \\~\\ \pause

But we didn't speak at all about selecting the observations themselves (assuming you have some modicum of control of selecting your data). Two things to consider: \\~\\  \pause

\begin{enumerate}
\item \emph{Generalizability} refers to the ability of the model to generalize, or be \emph{externally valid} when considering new observations. This comes down to sampling observations from the same population as your new data you wish to predict (pretty obvious). Sometimes difficult in practice! \pause
\item Optimal Design
\end{enumerate}

	
\end{frame}

\begin{frame}\frametitle{Optimal Design for Inferring one Slope}

Question: assume OLS and that we only care about inference for $\beta_1$. We can sample any $x$ values live in their set $\mathbb{X}$ e.g. $\in \bracks{x_m, x_M}$. What should the $n$ values be? \\~\\

Let $x_m = 0$, $x_M = 1$ and $n = 10$. The best inference for $\beta_1$ means ... \pause $\se{\betahat_1}$ is minimum. Design strategies for the $x$'s: \pause

\begin{enumerate}
\item Random sampling \pause
\item Uniform spacing: $\braces{0, 0.111, 0.222, \ldots, 0.999}$ \pause
\item Something else?
\end{enumerate}

[R demo]

\end{frame}


\begin{frame}\frametitle{Optimal Design: Split Between Extremes}

Recall the formula from Stat 102 / 613:

\beqn
\se{\betahat_1} = \sqrt{\frac{MSE}{(n-1)s^2_x}}
\eeqn

How can we make this small?

\begin{enumerate}
\item Maximize $n$ (duh)
\item Minimize the numerator, $MSE$ i.e. minimize the $SSE$. Can we do this? Yes by picking the closest $\betahat_1$ to $\beta_1$ (which we already do). \pause
\item Maximize the denominator $(n-1) s^2_x$. Since $n$ is already maximized, we can pick $x_1, \ldots, x_n$ to maximize $s^2_x$, the sample variance of the predictor.  How? \pause Put half of the $x$'s at $x_m$ and the other half at $x_M$ thereby \pause maximizing the distance from the $x$'s to \pause $\xbar$.
\end{enumerate}
	
\end{frame}

\begin{frame}\frametitle{Optimal Design of Linear Models}
\small

We seek the best linear approximation of $f(x)$ which is $\beta_0 + \beta_1 x _1+ \ldots + \beta_p x_p$. We pick the $\x$'s to give us the best linear approximation. What criteria? JMP gives two ways:

\begin{enumerate}
\item Note: $\var{\betahat_0, \betahat_1, \ldots, \betahat_p} = \sigsq \XtXinv$ \pause

$D$-optimality: maximize $\abss{\XtX}$ --- this maximizes the variance-covariance among the parameter estimates. \pause

\item Note: $\var{\Yhat_1, \ldots, \Yhat_n} = \sigsq \XXtXinvXt$ \pause

$I$-optimality: minimize the average prediction variance over the design space.
\end{enumerate}

[R Demo] \pause What did we learn? \pause For linear models with no polynomials or interactions, keep the observations as close to the minimimums and maximums as possible. For linear models with polynomials and interactions (more non-parametric than parametric), \pause keep most towards the minimums and maximums and some in the center of the input space.

\end{frame}


\section{Extrapolation}


\begin{frame}\frametitle{Extrapolation}
\small
Data driven approaches are all focused on accuracy during \emph{interpolation}.  

\begin{figure}
\centering
\includegraphics[width=2.5in]{extrap1.png}
\end{figure}
\vspace{-0.3cm}
Extrapolation brings trouble. It is important to ask the question for a new observation $\x^*$ if it is within the space of $\x$'s in the historical data. (Hardly anyone does this... but you should)! Be aware that extrapolation methods of different algorithms differ considerably! [R Demo]
	
\end{frame}

\begin{frame}\frametitle{Reconciliation of those Silly Cartoons}

\begin{figure}
\centering
\includegraphics[width=3.2in]{extrap.jpg}
\end{figure}

\end{frame}

\section{Logistic Regression}


\begin{frame}\frametitle{Modeling Categorical Responses}

Previously the response $y$ was continuous and via the OLS assumptions we obtained the statistical model,

\beqn
Y \inddist \normnot{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}{\sigsq}
\eeqn

If the response $y$ is categorical, can we still use this? \pause No... the only elements in the support of the r.v. $Y$ are the levels only. [JMP Churn] \\~\\

First, assume $Y$ is binary i.e. zero or one. The model we use is...\pause  

\beqn
Y \sim \bernoulli{f(x_1, \ldots, x_p)}
\eeqn

since $\cexpe{Y}{x_1, \ldots, x_p} = f(x_1, \ldots, x_p)$, then $f$ is still the conditional expectation function like before except now it varies only within \pause $\zeroonecl$ and it is the same as \pause   $\cprob{Y = 1}{x_1, \ldots, x_p}$.

\end{frame}

\begin{frame}\frametitle{Linear $f(x)$?}

We can model $f(x)$ as the simple linear function but this returns values smaller than 0 and larger than 1 and thus it cannot be the conditional expectation function! Why? \pause Lines vary between $\parens{-\infty, +\infty}$. \\~\\ \pause

We need a \qu{link function} to connect the linear function to the restricted support of the response:

\beqn
\lambda(f_{\reals}(x_1, \ldots, x_p)) = f(x_1, \ldots, x_p)
\eeqn

And the parametric assumption would be

\beqn
\lambda(s_{\reals}(x_1, \ldots, x_p; \theta_1, \ldots, \theta_\ell)) = s(x_1, \ldots, x_p; \theta_1, \ldots, \theta_\ell)
\eeqn

And assuming a linear form:

\beqn
\lambda(\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p) = ?
\eeqn

\end{frame}

\begin{frame}\frametitle{Choice of $\lambda$?}

We just need $\lambda: \reals \rightarrow \zeroonecl$. There are infinite $\lambda$'s to choose from. I've only seen three used: \pause

\begin{enumerate}
\item Logistic link: $\lambda(w) = \frac{e^w}{1 + e^w}$ (most common) \pause
\item Inverse normal (probit) link: $\lambda(w) = \Phi^{-1}(w)$ where $\Phi$ is the normal CDF function (somewhat common) \pause
\item Complementary Log-log (cloglog) link: $\lambda(w) = \natlog{-\natlog{w}}$ (rare!)
\end{enumerate}

Let's investigate what the first one means. Define $p := \prob{Y=1}$. We can think about probability  in another way:

\beqn
odds(Y = 1) := \frac{p}{1-p}
\eeqn

So if odds = 4:1, what is $p$? This means that the probability of the event happening is four times more likely than the complement happening. Or...  of 4+1 runs, 4 will be a yes. What is the range of odds? \pause $[0, \infty)$.
	
\end{frame}

\begin{frame}\frametitle{Why Logistic Link is Interpretable}
\small
Now let's take the log odds (called the logit function):

\beqn
logit(Y = 1) := \natlog{odds(Y = 1)} = \natlog{\frac{p}{1-p}}
\eeqn

What is the range of the logit function? \pause All of $\reals$. Hence, we can now set this equal to our \pause $s_\reals$ function. In the linear modeling context,

\footnotesize
\beqn
\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p &=& logit(Y = 1) =  \natlog{\frac{p}{1-p}} \\ \pause
e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p} &=& \frac{p}{1-p} \\ \pause
(1-p)e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p} &=& p \\ \pause
e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}  &=& p + p e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}\\ \pause
p &=& \frac{e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}} = \lambda(\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p) \pause
\eeqn

\small
Thus, a change in the linear model becomes a linear change in log-odds. This is (I would say) the most interpretable link function situation we've got.
	
\end{frame}

\begin{frame}\frametitle{The Logistic Function}

\begin{figure}
\centering
\includegraphics[width=3.2in]{logistic_function.png}
\end{figure}

\end{frame}

\begin{frame}\frametitle{How to Obtain a Model Fit}

A model fit would mean we estimate $\braces{\betahat_0, \betahat_1,  \ldots, \betahat_p}$. We initially did this estimation for regression (continuous $y$) by defining a loss function, SSE, and finding the optimal solution via calculus. What do we do now?? \\~\\

Likelihood to the rescue. First the \qu{logistic regression assumptions}

\begin{enumerate}
\item Linear-Logistic conditional expectation \pause
\item Independence
\end{enumerate}

\small
\beqn
&&\cprob{Y_1 = y_1, Y_2 = y_2, \ldots, Y_n = y_n}{\X_1 = \x_1, \X_2 = \x_2, \ldots, \X_n = \x_n} \\
&=& \prod_{i=1}^n \cprob{Y_i = y_i}{\X_1 = \x_i}
\eeqn

How?
	
\end{frame}

\begin{frame}\frametitle{Maximum Likelihood Estimates}
\small
\beqn
= \prod_{i=1}^n p^{y_i} (1-p)^{1-y_i}
\eeqn

How? \pause

\beqn
&& \lik{\beta_0, \beta_1, \ldots, \beta_p; \x_1, \ldots, \x_n} \\
&=& \prod_{i=1}^n \parens{\frac{e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}}^{y_i} \parens{1 - \frac{e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}}^{1-y_i}
\eeqn

How? \pause This does not have a simple, closed form solution. The computer iterates numerically usually using the log of above, since it's (1) numerically more stable and (2) the expression is easier to work with. When it \qu{converges} on the values of the parameters that maximize the above, these are shipped to you as $\braces{\betahat_0, \betahat_1,  \ldots, \betahat_p}$. This is called \qu{running a logistic regression}. This usually is instant on a modern computer.
	
\end{frame}

\begin{frame}\frametitle{Prediction with Logistic Regression}
\small

How? \pause

\beqn
\phat = \phat(x^*_1, \ldots, x^*_p) = \pause \frac{e^{\betahat_0 + \betahat_1 x_1 + \ldots + \betahat_p x_p}}{1 + e^{\betahat_0 + \betahat_1 x_1 + \ldots + \betahat_p x_p}}
\eeqn
	
Note the predictions are for the conditional expectation function, the probability itself. However, you may actually wish to predict the response, the 1 or the 0. What to do?\\~\\ \pause

You can create a \emph{classification rule} which allows you to make a decision about the response based on the probability. What is the most intuitive classification rule? \pause

\beqn
\yhat = \indic{\phat \geq 0.5} := \pause \begin{cases} 1 ~~~\text{if}~~~ \phat \geq 0.5 \\ 0 ~~~\text{if}~~~ \phat < 0.5 \end{cases}
\eeqn

AKA the \qu{most likely criterion}. \pause We will return to prediction and evaluation of predictive performance later but first...

\end{frame}


\begin{frame}\frametitle{Global Test in Logistic Regression}

\footnotesize
Recall in OLS regression, probability theory directly gave us $t$-tests and $F$-tests. Under the logistic regression assumptions, \emph{we have no such analogous theory}! However, we can make use of the ... \pause likelihood ratio test. Recall:

\vspace{-0.2cm}
\beqn
LR :=
%
\displaystyle \max_{\theta \in \Theta} \lik{\theta; x}
%
/
%
\displaystyle \max_{\theta \in \Theta_R}  \lik{\theta; x}
%
\eeqn

Let's now do a \qu{whole model} / \qu{global} / \qu{omnibus} test: \pause

\beqn
&& H_0: \beta_1 = 0, \beta_2 = 0, \ldots, \beta_p = 0, \quad H_a: \text{at least one is non-zero}
\eeqn

So $\Theta$ would be the space of all $\beta_0, \beta_1, \ldots, \beta_p$ and $\Theta_R$ will restrict the space to only $\beta_0$ with zeroes for all other \qu{slope} parameters.

\beqn
LR &=& \frac{
\displaystyle \max_{\beta_0, \beta_1, \ldots, \beta_p} 
\lik{\beta_0, \beta_1, \ldots, \beta_p; y_1, \ldots, y_n, \x_1, \ldots, \x_n}
}{
\displaystyle \max_{\beta_0} 
\lik{\beta_0, \beta_1 = 0, \ldots, \beta_p = 0; y_1, \ldots, y_n, \x_1, \ldots, \x_n}
}
\eeqn \pause


So on top the computer iterates to find $\braces{\betahat_0, \betahat_1,  \ldots, \betahat_p}$, plugs it in and computes the likelihood and on the bottom the computer independently iterates to find $\braces{\betahat_0}$, plugs it in and computes the likelihood, then together, the $LR$.
\end{frame}

\begin{frame}\frametitle{Partial Tests in Logistic Regression}

\small
We then look at $Q = 2\natlog{LR}$ and compare it to the appropriate $\chi^2$ distribution. Here, since we've dropped $p$ parameters / degrees of freedom, we look at the critical $\chisq{p, \alpha}$ value.\\~\\ \pause

Let's say we want to test something like:

\beqn
&& H_0: \beta_1 = 0, \beta_2 = 0, \quad H_a: \text{at least one is non-zero}
\eeqn

We can again use the likelihood ratio test:

\beqn
LR = \frac{
\displaystyle \max_{\beta_0, \beta_1, \ldots, \beta_p} 
\lik{\beta_0, \beta_1, \ldots, \beta_p; y_1, \ldots, y_n, \x_1, \ldots, \x_n}
}{
\displaystyle \max_{\beta_0, \beta_3, \ldots, \beta_p} 
\lik{\beta_0, \beta_1 = 0, \beta_2 = 0, \beta_3, \ldots, \beta_p = 0; y_1, \ldots, y_n, \x_1, \ldots, \x_n}
}
\eeqn \pause

We then look at $Q = 2\natlog{LR}$ and compare it to the appropriate $\chi^2$ distribution. Here, since we've dropped $2$ parameters / degrees of freedom, we look at the critical $\chisq{2, \alpha}$ value.\\~\\ \pause

\end{frame}

\begin{frame}\frametitle{Individual Tests in Logistic Regression}

Let's say we want to test an individual slope coefficient:

\beqn
&& H_0: \beta_j = 0, \quad H_a: \beta_j \neq 0
\eeqn
	
We can again use the likelihood ratio test:

\footnotesize
\beqn
\hspace{-10pt} LR = \frac{
\displaystyle \max_{\beta_0, \beta_1, \ldots, \beta_p} 
\lik{\beta_0, \beta_1, \ldots, \beta_p; y_1, \ldots, y_n, \x_1, \ldots, \x_n}
}{
\displaystyle \max_{\beta_0, \beta_1, \ldots, \beta_{j-1}, \beta_{j+1}, \ldots \beta_p} 
\lik{\beta_0, \beta_1, \ldots, \beta_{j-1}, \beta_j = 0, \beta_{j+1}, \ldots \beta_p; y_1, \ldots, y_n, \x_1, \ldots, \x_n}
}
\eeqn \pause

\small
We then look at $Q = 2\natlog{LR}$ and compare it to the appropriate $\chi^2$ distribution. Here, since we've dropped $2$ parameters / degrees of freedom, we look at the critical $\chisq{1, \alpha}$ value. \\~\\ \pause

There is something special about a $\chisq{}$ r.v. with one degree of freedom. \pause Cool fact from basic probability: $\sqrt{\chisq{1}} = Z \sim \stdnormnot$. This is how JMP produces standard errors for logistic regression coefficients.

\end{frame}


\begin{frame}\frametitle{Telecom Churn Example}

In marketing \qu{churn} refers to a customer canceling their service. Studies suggest that it costs 5-10x more to acquire a new customer than to retain an old customer. Thus, predicting churn is of major interest! \\~\\ \pause

Here's a dataset from a telecom company (likely it's churn on Verizon / AT\&T / T-Mobile /Sprint's cell-phone plan). We have 7,043 customers with 20 features. This is likely a nearly-mindless dump!! Churn is defined to be a complete cancellation of services in the next month period. Since we are predicting churn, define $y=1$ to be churn, so the $\phat$'s are estimates of probability of churning (this just makes everything easier to understand).
	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\section{Evaluating Classification Models}

\end{document}
	


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}






\begin{frame}\frametitle{}

	
\end{frame}



\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}