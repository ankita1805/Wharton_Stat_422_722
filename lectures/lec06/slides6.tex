%\documentclass[slides]{beamer} %switch "slides" to "handout" for printing out
\documentclass[handout]{beamer}

\include{preamble}

%presentation preamble
\usetheme{progressbar}
\usecolortheme{progressbar} 
\usefonttheme{progressbar} 
\useoutertheme{progressbar}
\useinnertheme{progressbar}

\title[Lecture]{Predictive Analytics Lecture 6}
\institute[Wharton, Statistics]{Stat 422/722\\ at The Wharton School of the University of Pennsylvania}
\date{February 21 \& 22, 2017}

\author{Adam Kapelner}


\begin{document}

%immediately create a title page
\frame{\titlepage}


\section{Automatic Model Selection}



\begin{frame}\frametitle{Modeling Framework Refresher}
\small
Recall the general regression model:

\beqn
Y = f(x_1, \ldots, x_p) + \errorrv
\eeqn

A couple lectures ago, we made the parametric assumption that: \pause

\beqn
Y = s(x_1, \ldots, x_p; \theta_1, \ldots, \theta_\ell) + \tilde{\errorrv}
\eeqn

where the $\tilde{\errorrv}$ term now includes the previous $\errorrv$ plus $f-s$, the misspecification error. \pause The parametric model $s$ we employed was the linear model and the $\theta$'s we called $\beta$'s: \pause

\beqn
Y = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p + \tilde{\errorrv}
\eeqn

Last lecture, we started adding interactions and polynomials (as well as other transformations e.g. log which we did not cover). \pause This was a means of \qu{expanding} the feature set \qu{visible} to the model using \qu{derived} features: \pause

\beqn
\braces{x_1, \ldots, x_p} \Rightarrow \braces{x'_1, \ldots, x'_{p'}} ~~ \text{where $p' > p$ and maybe much, much greater.}
\eeqn

\end{frame}

\begin{frame}\frametitle{\qu{Non-parametric} Linear Regression}

Once we expand this feature set, we can now fit a larger linear model:

\beqn
Y = \beta_0 + \beta_1 x'_1 + \ldots\ldots\ldots\ldots\ldots\ldots\ldots + \beta_p x'_{p'} + \tilde{\errorrv}
\eeqn

Given more degrees of freedom with this expanded feature set allows the linear model to fit more complicated real-world functions. \pause This is essentially a means of doing non-parametric parametric modeling (it's oxymoronic). It's technically parametric but conceptually it's non-parametric since we don't have our parametric benefits: parsimony, inference nor interpretation. Hopefully $\tilde{\errorrv}$ will be close to $\errorrv$, the irreducible noise. \\~\\ \pause

Back to our problem... we can curb overfitting by ... \pause using 3-way split oos validation but we need to select good models... how to do so? \pause One approach is termed \emph{subset selection methods}.
\end{frame}


\begin{frame}\frametitle{Stepwise Regression}

First we expand the feature set from $\braces{x_1, \ldots, x_p} \Rightarrow \braces{x'_1, \ldots, x'_{p'}}$. \pause Then we attempt to find the \qu{best} model consisting of a subset of these features. However there are $2^{p'}$ possible models. For $p'=20$ that's about 1,000,000. \pause So we try to find a model \textit{close} to the optimal using a \qu{heuristic} (a rule of thumb that seems to generally be useful).\\~\\ \pause

That heuristic is called \emph{stepwise} model construction. \pause We begin with \emph{forward stepwise} model construction: \pause

\begin{enumerate}
\item Find the \qu{best} feature from the list of expanded features.\pause
\item Find the \qu{next best} feature from the remaining expanded features.\pause
\item Repeat step 2 until you believe you are overfitting.\pause
\end{enumerate}




\end{frame}

\begin{frame}\frametitle{Estimating Overfitting (again)}
\small
If you choose the feature to give you the best in-sample $R^2$, you will eventually take all the features (until $n = p+1$) and you will get $R^2 = 100\%$. We need a metric to tell us when we may be overfitting and halt at that moment. \pause Here are a few:

\begin{enumerate}
\item oos RMSE (keep a holdout set and quit when this starts increasing) \pause
\item Only include a variable if its $t$ stat (or partial $F$ stat) is significant \pause
\item Use $AICc$.
\end{enumerate}

\beqn
-AIC = 2\loglik{\betahat; \y, \x} - 2p
\eeqn

The first component (the log-likelihood) represents in-sample fit. \pause $\loglik{}$ is like $R^2$ though... as the fit gets closer to the points, the likelihood goes to 1 (and the log likelihood goes to 0). \pause The $2p$ term is a reality check. If you have more features, you are going to overfit. So each additional feature must be justified in terms of the increase in log-likelihood. \pause Thus, good models maximize $-AIC$ (i.e. minimize $AIC$).\\~\\


	

\end{frame}

\begin{frame}\frametitle{$AICc$ for linear models}
\small
For linear regression under OLS, we can calculate the log-likelihood explicitly (we approximately did this in Lecture 2) to obtain:

\beqn
AIC = n\natlog{RMSE^2} + 2p
\eeqn

\pause So once again, we want this to be small. If we decrease $RMSE$ by adding a feature, it needs to counteract an increase of 2 by $p \rightarrow p+1$. If it can't, we're probably overfitting. \pause $AIC$ works well with large sample sizes. For small sample sizes, we use a corrected version $AICc$ defined as:

\beqn
AICc = AIC + \frac{2p(p+1)}{n-p-1}
\eeqn

Needless to say, this is all approximate since we are assuming OLS and a whole bunch of other things (beyond scope of course). \pause Note: there are also BIC and Mallow's $C_p$ which are similar metrics, but we will not cover them. \pause (JMP demo for white wine dataset and telecom set.)


	
\end{frame}

\begin{frame}\frametitle{More About Stepwise Linear Regression}

\small
\emph{Backward selection} begins with all features and then deletes one for each step until no more can justifiably be cut out. \pause Backward selection has a major weakness: it cannot be run on dataframes where the extended feature set is more than the number of rows (only forward or forward with mixed works there). \pause \emph{Mixed selection} begins with either none or all and then looks for both good additions and good subtractions. \\~\\ \pause

Simple case where stepwise doesn't work? \pause How about three features where $x_1$ is most correlated but $x_2$ and $x_3$ together are the best model but there is high collinearity between $x_2$ and $x_3$? What happens? \pause Forward: the model enters $x_1$ and then $x_2$ but it doesn't see $x_3$ as a worthy addition. Backward: the model can nuke $x_2$ or $x_3$ since its p-value or $F$ test is poor. \\~\\ \pause
	
This is not the only way to fit a flexible model and hedge against overfitting. We will do more such models. But first, we will talk about \qu{missing data} as it's more relevant to the project which is due soon.
\end{frame}



\end{document} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}