%\documentclass[slides]{beamer} %switch "slides" to "handout" for printing out
\documentclass[handout]{beamer}

\include{preamble}

%presentation preamble
\usetheme{progressbar}
\usecolortheme{progressbar} 
\usefonttheme{progressbar} 
\useoutertheme{progressbar}
\useinnertheme{progressbar}

\title[Lecture]{Predictive Analytics Lecture 5}
\institute[Wharton, Statistics]{Stat 422/722\\ at The Wharton School of the University of Pennsylvania}
\date{February 14 \& 15, 2017}

\author{Adam Kapelner}


\begin{document}

%immediately create a title page
\frame{\titlepage}


\begin{frame}\frametitle{Underfitting \& Overfitting}

\beqn
Y = f(x_1, \ldots, x_p) + \errorrv
\eeqn

Goal of machine learning: fit $f$ as best as possible. When we build models, we do one (or both of the following) \pause

\begin{itemize}
\item We fall \emph{short} by underfitting (usually due to too little degrees of freedom and inflexible bases). For example: if the $f$ is a curve and we fit a line, we underfit (recall medicorp sales vs bonus regression). \pause
\item We can shoot too \emph{long} by encroaching on and fitting / optimizing to the $\errorrv$. Since $\errorrv$ is independent of $x_1, \ldots, x_p$, this part of $\hat{f}$ is essentially a random fit and it is the opposite of the \qu{data-driven approach}.
\end{itemize}

	
\end{frame}

\begin{frame}\frametitle{Complexity-Fit Tradeoff}

\begin{figure}
\centering
\includegraphics[width=3.2in]{complexity_fit_tradeoff.png}
\end{figure}

\small
Blue is in-sample fit metric and red is oos fit metric. This is Fig 7.1 from Hastie and Tibsharani (2009).

%This provides an honest estimate for one model $\hat{f}$ and one model $\hat{f}$ only.

\end{frame}

\begin{frame}\frametitle{Assessment: OOS Validation}

But knowing where you are on that y-axis would involve knowing the truth. We need to estimate this, so we use oos validation:

\begin{figure}
\centering
\includegraphics[width=4.1in]{oos_validation}
\end{figure}

%This provides an honest estimate for one model $\hat{f}$ and one model $\hat{f}$ only.

\end{frame}



\begin{frame}\frametitle{Assumptions and Tradeoffs when Splitting}
\footnotesize
We have a choice to split our dataframe into two pieces. Assuming each data point is independent (the running assumption), you should do this completely randomly. When would this assumption not be true? \pause For example, a time series.\\~\\
\vspace{-0.2cm}

Additionally, we need to assume a non-stationary model relationship. So, 

\beqn
Y = f(x_1, \ldots, x_p) + \errorrv \quad \text{and not} \quad Y = f_t(x_1, \ldots, x_p) + \errorrv 
\eeqn

where $f$ changes with time. In essence non-stationarity is a lack of generalization and when predicting, it is a form of extrapolation. \\~\\

How large should the test set be? Usual sizes are 10-30\%. What's the tradeoff? If the test set is larger, then ...
 
\begin{enumerate}\footnotesize
\item the more accurate the assessment of generalization error would be (less variance) and
\item the less accurate the model will be since it's fitting with less data (more bias)
\end{enumerate}

\vspace{-0.1cm}
If the test set is smaller then, vice versa. Note: the in-sample and oos statistics are statistics! Thus, they are random!

\end{frame}


\section{Cross-Validation (CV)}

\begin{frame}\frametitle{Less Randomness in the OOS Statistics}
\small
If we change the observations in the training/test splits, we will get different models and different estimates of future error. \pause Thus, our oosRMSE was really oosRMSE conditional on the idiosyncratic split we happened to get!\\~\\ \pause 

We can at the very least... \pause  get rid of this idiosyncratic error by ... \pause averaging over all training-test splits. If we have $n=100$ and the test set is 10\%, that means we only have $\binom{100}{10} = 1.73 \times 10^{13}$ split configurations to average over! \\~\\ \pause 

\normalsize
We can approximate the averaging over all splits by just taking $\frac{100\%}{10\%} = 10$ random but unique splits called \emph{folds}. \pause Thus, each observation is represented in the test set once (leading to a more stable estimate). \pause This is known as \emph{$K$-fold cross validation (CV)} where here $K = \pause \frac{100\%}{10\%} = 10$ (and this procedure seems to be the industry standard).
	
\end{frame}


\begin{frame}\frametitle{10-fold CV}

\begin{figure}
\centering
\includegraphics[width=2.9in]{10_fold_cv2.png}
\end{figure}

\vspace{-0.3cm}
\small
$K=10$ is arbitrary (but remember where it was based on: the 10-30\% test set recommendation). In practice, I've only used 5 or 10 fold CV. \\~\\ \pause

\vspace{-0.3cm}
This does not really solve any of our big problems but gives us a little boost in terms of a reduction in standard error of our generalization error estimate. \pause That's OK; we can take all the help we can get if it's costless!! \\~\\

\vspace{-0.4cm}\footnotesize
Note 1: If $K=n$ then we the test set as one sample; this is known as \qu{leave on out CV} (LOOCV) and it not recommended --- high variance and \pause computat- ional cost! \pause Note 2: bleeding edge of stats --- find CI's for generalization error. \pause Note 4: this is not the only way to reduce the variance in oos statistics but it's the one we will use in this class.

	
\end{frame}


\begin{frame}\frametitle{$K=3$-fold CV on a Linear Model Ex. 1/5}

Imagine the following data $n=9$ where we are fitting a response by one feature (ignore the colors):

\begin{figure}
\centering
\includegraphics[width=2.1in]{data.PNG}
\end{figure}

Imagine we choose a linear model. 
	
\end{frame}


\begin{frame}\frametitle{$K=3$-fold CV on a Linear Model Ex. 2/5}

In the first fold, the red is left out and thus we fit a line to the blue and green points:

\begin{figure}
\centering
\includegraphics[width=1.8in]{l1.PNG}
\end{figure}

Then we calculate the residuals to the red points (the test set in this fold) and calculate

\beqn
SSE = (2 - 2.2)^2 + (3.8 - 2.1)^2 + (3.5 - 2.05)^2 = 5.03
\eeqn

\end{frame}


\begin{frame}\frametitle{$K=3$-fold CV on a Linear Model Ex. 3/5}

In the second fold, the green is left out and thus we fit a line to the blue and red points:

\begin{figure}
\centering
\includegraphics[width=1.8in]{l2.PNG}
\end{figure}

Then we calculate the residuals to the red points (the test set in this fold) and calculate

\beqn
SSE = (1.2 - 2.3)^2 + (3.4 - 2.25)^2 + (1.3 - 2.2)^2 = 3.34
\eeqn
	
\end{frame}


\begin{frame}\frametitle{$K=3$-fold CV on a Linear Model Ex. 4/5}

In the third (last) fold, the blue is left out and thus we fit a line to the green and red points:

\begin{figure}
\centering
\includegraphics[width=1.8in]{l3.PNG}
\end{figure}

Then we calculate the residuals to the red points (the test set in this fold) and calculate

\beqn
SSE = (2.35 - 2.3)^2 + (2.4 - 1.5)^2 + (2.2 - 1.4)^2 = 1.45
\eeqn
	
\end{frame}


\begin{frame}\frametitle{$K=3$-fold CV on a Linear Model Ex. 1/5}

Then we aggregate all oos results together (SSE's are additive) and we can compute a final oos statistics e.g. the oosRMSE:

\beqn
oosRMSE = \sqrt{\frac{5.03 + 3.34 + 1.45}{9}} = 1.045
\eeqn
	
\end{frame}

\begin{frame}\frametitle{Limits of JMP / Intro to R's MLR Package}

And... \pause JMP can't do $K$-fold CV! (Except in one limited case which doesn't help us right now). But of course R can do it... [R Demo with MLR]
	
\end{frame}


\begin{frame}\frametitle{Validating Multiple Models}
\footnotesize
Let's look at a few models for the White Wine data with no validation (but no cross-validation). Here the response is wine quality as measured by professional raters and features are 11 features (e.g. acidity, sugar, pH and alcohol content).

\vspace{-0.1cm}
\begin{enumerate}\footnotesize
\item[A] plain linear model \pause
\item[B] 6-degree polynomials for all features \pause
\item[C] 6$^{\text{o}}$ polynomials and all 1st order interactions  \pause
\item[D] 6$^{\text{o}}$ polynomials and all interactions up to 3rd degree (AKA 2nd order) \pause
\item[E] 6$^{\text{o}}$ polynomials and all interactions up to 4th degree (AKA 3rd order) \pause
\item[F] 6$^{\text{o}}$ polynomials and all interactions up to 11th order \pause
\end{enumerate}
\footnotesize
\vspace{-0.1cm}
[JMP col validation... fit all models with validation ... save prediction formula cols... analyze model... model comparison... \pause complexity tradeoff illustration] \pause Conclusions? \pause Model C looks the best. \pause Note: another popular assessment metric besides oosRMSE is oosAAE which is just average absoute value difference. Strange ... given that linear models optimize for squared error. \pause 
Show Demo with MLR w/ 10-fold CV \pause \inred{What \textit{precisely} did I do that wasn't legal?} 
	
\end{frame}

\begin{frame}\frametitle{A Possible Spin on Validation}
\small

Recall the proposal from last class:
	
\begin{enumerate}\footnotesize
\item Split dataframe into training and test.
\item Build model A on training.
\item Predict using the test set. 
\item Calculate estimate of future generalization error of model 1.
\item Build a different model B on training.
\item Predict using the test set. 
\item Calculate estimate of future generalization error of model 2.
\item ... steps 5-7 for model 3
\item ... steps 5-7 for model 4
\item ...
\item ... steps 5-7 for model M
\item Pick whichever model has better generalization error.
\end{enumerate}

This is a form of \emph{Model Selection}. What was wrong with it?

\end{frame}

\begin{frame}\frametitle{Looking into the Future is Not Legal}

The oos validation is only valid if...

\begin{figure}
\centering
\includegraphics[width=1.2in]{lockbox.png}
\end{figure}

you treat the test set as a lockbox. Once you open it up, that's it! And we opened it up $M$ times!\\~\\ \pause

This is indeed a \emph{Model Selection} procedure but ... \pause our estimate of future generalization error is invalid. How can we do both?
	
\end{frame}

\section{Three Splits and CV}

\begin{frame}\frametitle{In One Fold Let's Focus on the Training Set}
\vspace{-0.2cm}
\begin{figure}
\centering
\includegraphics[width=3.1in]{oos_validation_highlighted}
\end{figure}

\small
\vspace{-0.3cm}
\pause This procedure was completely valid as long as we did not touch the test set, right? \pause As long as we operate only within the training set... we're OK!
	
\end{frame}

\begin{frame}\frametitle{Training $\Rightarrow$ Training \& Validation}
\vspace{-0.5cm}
\begin{figure}
\centering
\includegraphics[width=4.4in]{training_validation}
\end{figure}
	
\end{frame}

\begin{frame}\frametitle{3-way Splitting: The Full Picture}
\vspace{-0.8cm}
\begin{figure}
\centering
\includegraphics[width=4.26in]{3_way_validation}
\end{figure}
	
\end{frame}

\begin{frame}\frametitle{Why Training-Validation-Test Splitting?}

\begin{itemize}
\item Training set: \pause provides fits for many models where overfitting is \qu{okay}
\item Validation set: \pause provides out-of-sample validation for each of the models. If the models are overfit, they will get wrecked at this stage.
\item Test set: \pause this lockbox provides a layer of security against overfitting within the training-validation union set. \pause
\end{itemize}

Just like in the previous 2-way training-test split, you can overfit the training, get killed on the test set and be stuck. How could you similaryl overfit here? \pause Be careful of optimizing to the validation set. Models $1, 2, \ldots, M$ should still be reasonable thought-through models.


\end{frame}


\begin{frame}\frametitle{Sizes of Training-Validation-Test Splits}

Previously, we had the test set being 10-30\%. This recommendation remains. Thus, the training-validation sets together should make up 70-90\% of which a portion is the validation set \pause (which is like a test set) and should be \pause 10-30\% of the total. \pause Thus we arrive at proportions like 50-25-25 or 50-30-20 or 70-20-10. \pause There is no exact guidance here. \\~\\ \pause

The same tradeoffs apply to the test set size but now we have new tradeoffs for the training set size vs. the validation set size:

\begin{itemize}
\item The larger the training set, \pause the better the fit of the model (less bias) but the more variance in its assessment versus its peers
\item The smaller the training set, \pause the worse the fit of the model (more bias) but the less variance in its assessment versus its peers
\end{itemize}
	
\end{frame}

\begin{frame}\frametitle{Back to Wine...}

We can do a single 3-partition split in JMP using the validation column... then data filter... then fit the 6 models again... then use model comparison to select best model ... then undo the filter ... then use model comparison again to find our test set error (the guess of the generalization error)... then build the full model for public consumption.
	
\end{frame}

\begin{frame}\frametitle{Can you CV this 3-Split Procedure?}

\pause

\begin{minipage}{0.6\textwidth}
\begin{figure}
\centering
\includegraphics[width=2.56in]{nested_resampling}
\end{figure}
\end{minipage}~
\begin{minipage}{0.35\textwidth}
\small
Yes. It is called \qu{nested resampling} and $K=10$ fold CV is illustrated here. But... \pause what are you now evaluating? \\~\\

\pause Without CV, it was just model $m^*$. But here $m^*$ varies with the 10 folds! \pause You are testing the entire procedure i.e. given models $1, 2, \ldots, M$, pick the best one and ship it. How well you do in the future is estimated by the CV test set.
\end{minipage}


	
\end{frame}



\begin{frame}\frametitle{Not the First Means for Model Selection}

CV with nested resampling is not generally done in the way it was described here as a means to evaluate a model selection procedure. \pause Beyond scope of course: it is usually done to compare tuning settings in a non-parametric machine learning algorithm. We will see what this means next class. \\~\\ \pause

Neither R nor JMP (to my knowledge) can do this out-of-the-box. In R, even with MLR, you have to program it (MLR uses it for tuning an algorithm). Maybe I will write to them?
	
\end{frame}

\begin{frame}\frametitle{Nested Resampling for Tuning}

\begin{figure}
\centering
\includegraphics[width=3.56in]{nested_resampling_for_tuning.png}
\end{figure}

(from \href{https://mlr-org.github.io/mlr-tutorial/devel/html/nested_resampling/index.html}{MLR's tutorial website}).
\end{frame}

\section{Automatic Model Selection}

\begin{frame}\frametitle{3-way splits for model selection \& evaluation}
\small
Forget CV for a moment since it complicates things ... the \qu{inner split} consisted of the training-validation. We used this to \qu{select} a model based on lowest oos error (RMSE, or highest $R^2$). 

\vspace{-0.2cm}
\begin{figure}
\centering
\includegraphics[width=2.1in]{training_validation}
\end{figure}
\vspace{-0.2cm}
What's the cost? \pause We had to use only 70-90\% of the data (save the test set) to build the model. But what's the bigger cost?? \pause You had to provide models $1, 2, \ldots, M$!! This is no trivial task. So far I've been making them up! I've been using elebaorate linear models which pivoted from param- etric to non-parametric with the addition of curves and interactions. Forget finding the best model... how do we know we even employed good candidates?? \pause \inred{We don't!}

\end{frame}


\begin{frame}\frametitle{Modeling Framework Refresher}
\small
Recall the general regression model:

\beqn
Y = f(x_1, \ldots, x_p) + \errorrv
\eeqn

A couple lectures ago, we made the parametric assumption that: \pause

\beqn
Y = s(x_1, \ldots, x_p; \theta_1, \ldots, \theta_\ell) + \tilde{\errorrv}
\eeqn

where the $\tilde{\errorrv}$ term now includes the previous $\errorrv$ plus $f-s$, the misspecification error. \pause The parametric model $s$ we employed was the linear model and the $\theta$'s we called $\beta$'s: \pause

\beqn
Y = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p + \tilde{\errorrv}
\eeqn

Last lecture, we started adding interactions and polynomials (as well as other transformations e.g. log which we did not cover). \pause This was a means of \qu{expanding} the feature set \qu{visible} to the model using \qu{derived} features: \pause

\beqn
\braces{x_1, \ldots, x_p} \Rightarrow \braces{x'_1, \ldots, x'_{p'}} ~~ \text{where $p' > p$ and maybe much, much greater}
\eeqn

\end{frame}

\begin{frame}\frametitle{\qu{Non-parametric} Linear Regression}

Once we expand this feature set, we can now fit a larger linear model:

\beqn
Y = \beta_0 + \beta_1 x'_1 + \ldots\ldots\ldots\ldots\ldots\ldots\ldots + \beta_p x'_{p'} + \tilde{\errorrv}
\eeqn

Given more degrees of freedom with this expanded feature set allows the linear model to fit more complicated real-world functions. \pause This is essentially a means of doing non-parametric parametric modeling (it's oxymoronic). It's technically parametric but conceptually it's non-parametric since we don't have our parametric benefits: parsimony, inference nor interpretation. Hopefully $\tilde{\errorrv}$ will be close to $\errorrv$, the irreducible noise. \\~\\ \pause

Back to our problem... we can curb overfitting by ... \pause using 3-way split oos validation but we need to select good models... how to do so? \pause One approach is termed \emph{subset selection methods}.
\end{frame}


\begin{frame}\frametitle{Stepwise Regression}

First we expand the feature set from $\braces{x_1, \ldots, x_p} \Rightarrow \braces{x'_1, \ldots, x'_{p'}}$. \pause Then we attempt to find the \qu{best} model consisting of a subset of these features. However there are $2^{p'}$ possible models. For $p'=20$ that's about 1,000,000. \pause So we try to find a model \textit{close} to the optimal using a \qu{heuristic} (a rule of thumb that seems to generally be useful).\\~\\ \pause

That heuristic is called \emph{stepwise} model construction. \pause We begin with \emph{forward stepwise} model construction: \pause

\begin{enumerate}
\item Find the \qu{best} feature from the list of expanded features.\pause
\item Find the \qu{next best} feature from the remaining expanded features.\pause
\item Repeat step 2 until you believe you are overfitting.\pause
\end{enumerate}




\end{frame}

\begin{frame}\frametitle{Estimating Overfitting (again)}
\small
If you choose the feature to give you the best in-sample $R^2$, you will eventually take all the features (until $n = p+1$) and you will get $R^2 = 100\%$. We need a metric to tell us when we may be overfitting and halt at that moment. \pause Here are a few:

\begin{enumerate}
\item oos RMSE (keep a holdout set and quit when this starts increasing) \pause
\item Only include a variable if its $t$ stat (or partial $F$ stat) is significant \pause
\item Use $AICc$.
\end{enumerate}

\beqn
-AIC = 2\loglik{\betahat; \y, \x} - 2p
\eeqn

The first component (the log-likelihood) represents in-sample fit. \pause $\loglik{}$ is like $R^2$ though... as the fit gets closer to the points, the likelihood goes to 1 (and the log likelihood goes to 0). \pause The $2p$ term is a reality check. If you have more features, you are going to overfit. So each additional feature must be justified in terms of the increase in log-likelihood. \pause Thus, good models maximize $-AIC$ (i.e. minimize $AIC$).\\~\\


	

\end{frame}

\begin{frame}\frametitle{$AICc$ for linear models}
\small
For linear regression under OLS, we can calculate the log-likelihood explicitly (we approximately did this in Lecture 2) to obtain:

\beqn
AIC = n\natlog{RMSE^2} + 2p
\eeqn

\pause So once again, we want this to be small. If we decrease $RMSE$ by adding a feature, it needs to counteract an increase of 2 by $p \rightarrow p+1$. If it can't, we're probably overfitting. \pause $AIC$ works well with large sample sizes. For small sample sizes, we use a corrected version $AICc$ defined as:

\beqn
AICc = AIC + \frac{2p(p+1)}{n-p-1}
\eeqn

Needless to say, this is all approximate since we are assuming OLS and a whole bunch of other things (beyond scope of course). \pause Note: there are also BIC and Mallow's $C_p$ which are similar metrics, but we will not cover them. \pause JMP demo for white wine dataset.
	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\end{document} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}