%\documentclass[slides]{beamer} %switch "slides" to "handout" for printing out
\documentclass[handout]{beamer}

\include{preamble}

%presentation preamble
\usetheme{progressbar}
\usecolortheme{progressbar} 
\usefonttheme{progressbar} 
\useoutertheme{progressbar}
\useinnertheme{progressbar}

\title[Lecture]{Predictive Analytics Lecture 5}
\institute[Wharton, Statistics]{Stat 422/722\\ at The Wharton School of the University of Pennsylvania}
\date{February 14 \& 15, 2017}

\author{Adam Kapelner}


\begin{document}

%immediately create a title page
\frame{\titlepage}

\section{Missing Data}


\begin{frame}\frametitle{The Scourge of Modeling}
\small
Finding a good model selection is the big tough problem of statistics ... but missingness is the \qu{scourge} of data analysis and modeling. It's a big nuisance which has to be dealt with in order to get to inference and prediction. What is missingness? \pause Show JMP class project prediction dataframe. \\~\\ \pause

Simply put, missingness is the absence of the measurement for an observation (a hole in the matrix!) I will cover some theory of missingness, of which you will not need to know. But you will need to know the eventual takeaway messages. \\~\\ \pause

We will denote the full data matrix as $\X := \bracks{\x_1, \ldots, \x_p}$ where each $\x$ has length $n$. We will denote the observed data matrix as $\X_{obs} := \bracks{\x_{1,obs}, \ldots, \x_{p,obs}}$ as well as the missingness features as $\M = \bracks{\m_1, \ldots, \m_p}$. \\~\\ \pause

Previously the goal was to fit $f(x_1,\ldots, x_p)$ using $\X$ now we only have $\X_{obs}$ and $\M$.

	
\end{frame}


\begin{frame}\frametitle{Types of Missingness Models}

There are two ways to view response models with missingness. Given the entire dataset is seen we have:
\begin{enumerate}
\item \emph{Selection} Models. The repsonse is independent of missingness.\\ \pause 
\footnotesize
Imagine we are predicting whether or not a person will default on their mortgage. The person fills out an application for a mortgage in a bank. One of the fields on the paper form elicits their age. If a person forgot to fill out their age (there are many fields and this one just slipped by them), age will be missing on the final form. This will indeed impact the ability of the built model to predict loan payback (we will discuss workarounds soon), but there is no reason to suspect that there will be a separate model for those who forget to fill it out and those who don't. \pause

\item \emph{Pattern Mixture} Models. The response is not independent of missingness.\\ \pause
\footnotesize
Same modeling context but here we have the age filled out by a computerized background check which scours public and FBI records. If age goes missing here... that means... \pause the person may have some legal issues going on. It would make sense to fit two models: $f(x, m = 0)$ and $f(x, m = 1)$. Likely, $\phat$ will be higher in the latter!
%\beqn
%\cprob{\Y, \M}{\X, \thetavec, \gammavec} = \cprob{\Y}{\X, \thetavec} \cprob{\M}{\X, \gammavec}
%\eeqn

%Here, 
%
%\beqn
%\cprob{\Y, \M}{\X, \thetavec, \gammavec} = \cprob{\Y}{\M, \X, \thetavec} \cprob{\M}{\X, \gammavec}.
%\eeqn
\end{enumerate}


\end{frame}


\begin{frame}\frametitle{Default to Pattern Mixture Model}

In the latter, missingness itself affects the response. Unless you have a good reason to believe otherwise (you would need to think about how each variable went missing), you should default to thinking about it as a pattern mixture model.\\~\\  \pause

In the mortgage application example, the fact that age went missing needs to become a feature in its own right. \pause This is handled (usually) by including the dummy variables $\m_1, \ldots, \m_p$ and then... \pause interacting these dummies with other features to allow for differential response models based on missingness.
	
\end{frame}

\begin{frame}\frametitle{Night and Day Strategies to Fill in the Holes}
\footnotesize
The dichotomy of selection vs. pattern mixture is one piece of the puzzle. Now you have the practical piece of the puzzle --- your dataframe has holes!! What to do?\pause

\begin{enumerate} \footnotesize
\item \emph{Listwise Deletion} \pause --- simply put, delete all rows with missingness. Why is this bad? \pause Selection Bias!!! This bias will lead to poor future predictive performance (bad generalization). Also, even if there is no selection bias, ... \pause you are throwing out precious information! Your model could have been better if you figure out a principled means to make use of the measurements that you \textit{do} have on those observations.
\item \emph{Imputation} \pause --- \qu{fill in} the holes with \qu{good guesses}. \pause This essentially means you must create a model for holes in each measurement. What is the simplest imputation strategy (i.e. what is the simplest model)? \pause The average! \pause
\end{enumerate}

By default, when producing a model, JMP will use listwise deletion (don't do this!!). \pause I you are less lazy, you can generate a dummy columns, i.e. $\m$, and impute by using $\xbar$ (better than nothing). Even better is to create a model to do the imputation by treating the $x$ as the response (meta dude!) and the other $x's$ as the predictors. There are other ideas beyond the scope of the course!

\end{frame}

\begin{frame}\frametitle{Missingness Mechanisms}

Good imputation requires you to know what's going on i.e. how measurements wound up missing. Statisticians split this space up into three categories called \emph{missingness data mechanisms} (MDM): \pause (1) Missing Completely at Random (MCAR) \pause (2) Missing at Random (MAR) and \pause (3) Not Missing at Random (NMAR) and they are defined in the following way:\pause
	
	
\begin{table}[htp]
\centering
\begin{tabular}{l|l}
MDM & $\cprob{\M_j}{X_{j,\text{miss}}, \Xminjmiss, \Xminjobs, \gammavec} = \ldots$ \\ \hline
MCAR & $\cprob{\M_j}{\gammavec}$ \\ 
MAR & $\cprob{\M_j}{\Xminjmiss, \Xminjobs, \gammavec}$ \\ 
NMAR & (does not simplify)
\end{tabular}
\end{table}
\end{frame}

Conceptually speaking, let's take missing the age field as an example. 




\begin{frame}\frametitle{Conceptual Understanding of the MDM's}
\vspace{-0.25cm}
\small
\begin{enumerate}
\item MCAR means the missingness has nothing to do with any of the characteristics of the person. Is this likely? \pause No. You can think of MCAR as a completely random computer glitch that causes holes (MCAR scenario is usually rare). Are MCAR holes imputable? \pause No. Strategy? \pause Use $\xbar$. \pause \vspace{-0.1cm}
\item MAR means the missingness is based on other \textit{known} predictors. Thus, imputation is essentially a regression problem (or classification problem if the $x$ that went missing is categorical) where $y$ is one of the features (meta!). MAR holes are the most imputable! \pause Could you think of a reason why that would be in this example? \pause Likely not... Can you think of another example? \pause MAR is imputable! \pause \vspace{-0.1cm}
\item NMAR means the missingness is based on \textit{unknown features} and/or \textit{the value of this feature itself that we don't know}. \pause Is that the case here? \pause Likely... older people tend to forget to fill in fields... also... maybe the person is hiding something (in which case we are in the pattern mixture scenario). \pause NMAR is likely the most common and it is not so imputable... \pause but people still pretend it's MAR and attempt to impute... likely you're doing better than just using $\xbar$.
\end{enumerate}
\end{frame}


\begin{frame}\frametitle{Missingness in Future Measurements}

As we see in the prediction dataset, there is missingness in the $x^*$'s i.e. the observations you wish to predict on in the future. \pause If you are building an imputation model based on the other features, you can row-join both the historical dataframe and the $x^*$ data frame to build the models. \pause Also, make sure you create the $\m$ dummy variables for the same variables that had $\m$ dummy variables in the historical dataframe! \\~\\ \pause

Also... obvious but must mention it... listwise deletion is not an option for the $\x^*$'s! You cannot say, \qu{sorry I don't want to predict for this observation}!! You need to impute --- it's your job!

\end{frame}

\begin{frame}\frametitle{Missingness Takeaways}

Studying best ways to handle missingness can take a whole semester (some people have spent their entire careers on it), but for the purposes of this class we have a few takeaways: \pause

\begin{itemize}
\item Create a missingness dummy column $\m$ and use it in models with appropriate interactions to hedge against pattern mixture models. \pause
\item Attempt to reason through missingness for each measurement separately. \pause
\item If MCAR, use $\xbar$ to impute holes; if MAR / NMAR, use a prediction model to impute holes \pause (if you are supremely lazy, use $\xbar$).
\item Delete observations that have missingness \inred{at your peril!!}
\end{itemize}
	
\end{frame}

\section{Overfitting Review}

\begin{frame}\frametitle{Underfitting \& Overfitting}

\beqn
Y = f(x_1, \ldots, x_p) + \errorrv
\eeqn

Goal of machine learning: fit $f$ as best as possible. When we build models, we do one (or both of the following) \pause

\begin{itemize}
\item We fall \emph{short} by underfitting (usually due to too little degrees of freedom and inflexible bases). For example: if the $f$ is a curve and we fit a line, we underfit (recall medicorp sales vs bonus regression). \pause
\item We can shoot too \emph{long} by encroaching on and fitting / optimizing to the $\errorrv$. Since $\errorrv$ is independent of $x_1, \ldots, x_p$, this part of $\hat{f}$ is essentially a random fit and it is the opposite of the \qu{data-driven approach}.
\end{itemize}

	
\end{frame}

\begin{frame}\frametitle{Complexity-Fit Tradeoff}

\begin{figure}
\centering
\includegraphics[width=3.2in]{complexity_fit_tradeoff.png}
\end{figure}

\small
Blue is in-sample fit metric and red is oos fit metric. This is Fig 7.1 from Hastie and Tibsharani (2009).

%This provides an honest estimate for one model $\hat{f}$ and one model $\hat{f}$ only.

\end{frame}

\begin{frame}\frametitle{Assessment: OOS Validation}

But knowing where you are on that y-axis would involve knowing the truth. We need to estimate this, so we use oos validation:

\begin{figure}
\centering
\includegraphics[width=4.1in]{oos_validation}
\end{figure}

%This provides an honest estimate for one model $\hat{f}$ and one model $\hat{f}$ only.

\end{frame}



\begin{frame}\frametitle{Assumptions and Tradeoffs when Splitting}
\footnotesize
We have a choice to split our dataframe into two pieces. Assuming each data point is independent (the running assumption), you should do this completely randomly. When would this assumption not be true? \pause For example, a time series.\\~\\
\vspace{-0.2cm}

Additionally, we need to assume a non-stationary model relationship. So, 

\beqn
Y = f(x_1, \ldots, x_p) + \errorrv \quad \text{and not} \quad Y = f_t(x_1, \ldots, x_p) + \errorrv 
\eeqn

where $f$ changes with time. In essence non-stationarity is a lack of generalization and when predicting, it is a form of extrapolation. \\~\\

How large should the test set be? Usual sizes are 10-30\%. What's the tradeoff? If the test set is larger, then ...
 
\begin{enumerate}\footnotesize
\item the more accurate the assessment of generalization error would be (less variance) and
\item the less accurate the model will be since it's fitting with less data (more bias)
\end{enumerate}

\vspace{-0.1cm}
If the test set is smaller then, vice versa. Note: the in-sample and oos statistics are statistics! Thus, they are random!

\end{frame}


\section{Cross-Validation (CV)}

\begin{frame}\frametitle{Less Randomness in the OOS Statistics}
\small
If we change the observations in the training/test splits, we will get different models and different estimates of future error. \pause Thus, our oosRMSE was really oosRMSE conditional on the idiosyncratic split we happened to get!\\~\\ \pause 

We can at the very least... \pause  get rid of this idiosyncratic error by ... \pause averaging over all training-test splits. If we have $n=100$ and the test set is 10\%, that means we only have $\binom{100}{10} = 1.73 \times 10^{13}$ split configurations to average over! \\~\\ \pause 

\normalsize
We can approximate the averaging over all splits by just taking $\frac{100\%}{10\%} = 10$ random but unique splits called \emph{folds}. \pause Thus, each observation is represented in the test set once (leading to a more stable estimate). \pause This is known as \emph{$K$-fold cross validation (CV)} where here $K = \pause \frac{100\%}{10\%} = 10$ (and this procedure seems to be the industry standard).
	
\end{frame}


\begin{frame}\frametitle{10-fold CV}

\begin{figure}
\centering
\includegraphics[width=2.9in]{10_fold_cv2.png}
\end{figure}

\vspace{-0.3cm}
\small
$K=10$ is arbitrary (but remember where it was based on: the 10-30\% test set recommendation). In practice, I've only used 5 or 10 fold CV. \\~\\ \pause

\vspace{-0.3cm}
This does not really solve any of our big problems but gives us a little boost in terms of a reduction in standard error of our generalization error estimate. \pause That's OK; we can take all the help we can get if it's costless!! \\~\\

\vspace{-0.4cm}\footnotesize
Note 1: If $K=n$ then we the test set as one sample; this is known as \qu{leave on out CV} (LOOCV) and it not recommended --- high variance and \pause computat- ional cost! \pause Note 2: bleeding edge of stats --- find CI's for generalization error. \pause Note 4: this is not the only way to reduce the variance in oos statistics but it's the one we will use in this class.

	
\end{frame}


\begin{frame}\frametitle{$K=3$-fold CV on a Linear Model Ex. 1/5}

Imagine the following data $n=9$ where we are fitting a response by one feature (ignore the colors):

\begin{figure}
\centering
\includegraphics[width=2.1in]{data.PNG}
\end{figure}

Imagine we choose a linear model. 
	
\end{frame}


\begin{frame}\frametitle{$K=3$-fold CV on a Linear Model Ex. 2/5}

In the first fold, the red is left out and thus we fit a line to the blue and green points:

\begin{figure}
\centering
\includegraphics[width=1.8in]{l1.PNG}
\end{figure}

Then we calculate the residuals to the red points (the test set in this fold) and calculate

\beqn
SSE = (2 - 2.2)^2 + (3.8 - 2.1)^2 + (3.5 - 2.05)^2 = 5.03
\eeqn

\end{frame}


\begin{frame}\frametitle{$K=3$-fold CV on a Linear Model Ex. 3/5}

In the second fold, the green is left out and thus we fit a line to the blue and red points:

\begin{figure}
\centering
\includegraphics[width=1.8in]{l2.PNG}
\end{figure}

Then we calculate the residuals to the red points (the test set in this fold) and calculate

\beqn
SSE = (1.2 - 2.3)^2 + (3.4 - 2.25)^2 + (1.3 - 2.2)^2 = 3.34
\eeqn
	
\end{frame}


\begin{frame}\frametitle{$K=3$-fold CV on a Linear Model Ex. 4/5}

In the third (last) fold, the blue is left out and thus we fit a line to the green and red points:

\begin{figure}
\centering
\includegraphics[width=1.8in]{l3.PNG}
\end{figure}

Then we calculate the residuals to the red points (the test set in this fold) and calculate

\beqn
SSE = (2.35 - 2.3)^2 + (2.4 - 1.5)^2 + (2.2 - 1.4)^2 = 1.45
\eeqn
	
\end{frame}


\begin{frame}\frametitle{$K=3$-fold CV on a Linear Model Ex. 1/5}

Then we aggregate all oos results together (SSE's are additive) and we can compute a final oos statistics e.g. the oosRMSE:

\beqn
oosRMSE = \sqrt{\frac{5.03 + 3.34 + 1.45}{9}} = 1.045
\eeqn
	
\end{frame}

\begin{frame}\frametitle{Limits of JMP / Intro to R's MLR Package}

And... \pause JMP can't do $K$-fold CV! (Except in one limited case which doesn't help us right now). But of course R can do it... [R Demo with MLR]
	
\end{frame}


\begin{frame}\frametitle{Validating Multiple Models}
\footnotesize
Let's look at a few models for the White Wine data with no validation (but no cross-validation). Here the response is wine quality as measured by professional raters and features are 11 features (e.g. acidity, sugar, pH and alcohol content).

\vspace{-0.1cm}
\begin{enumerate}\footnotesize
\item[A] plain linear model \pause
\item[B] 6-degree polynomials for all features \pause
\item[C] 6$^{\text{o}}$ polynomials and all 1st order interactions  \pause
\item[D] 6$^{\text{o}}$ polynomials and all interactions up to 3rd degree (AKA 2nd order) \pause
\item[E] 6$^{\text{o}}$ polynomials and all interactions up to 4th degree (AKA 3rd order) \pause
\item[F] 6$^{\text{o}}$ polynomials and all interactions up to 11th order \pause
\end{enumerate}
\footnotesize
\vspace{-0.1cm}
[JMP col validation... fit all models with validation ... save prediction formula cols... analyze model... model comparison... \pause complexity tradeoff illustration] \pause Conclusions? \pause Model C looks the best. \pause Note: another popular assessment metric besides oosRMSE is oosAAE which is just average absoute value difference. Strange ... given that linear models optimize for squared error. \pause 
Show Demo with MLR w/ 10-fold CV \pause \inred{What \textit{precisely} did I do that wasn't legal?} 
	
\end{frame}

\begin{frame}\frametitle{A Possible Spin on Validation}
\small

Recall the proposal from last class:
	
\begin{enumerate}\footnotesize
\item Split dataframe into training and test.
\item Build model A on training.
\item Predict using the test set. 
\item Calculate estimate of future generalization error of model 1.
\item Build a different model B on training.
\item Predict using the test set. 
\item Calculate estimate of future generalization error of model 2.
\item ... steps 5-7 for model 3
\item ... steps 5-7 for model 4
\item ...
\item ... steps 5-7 for model M
\item Pick whichever model has better generalization error.
\end{enumerate}

This is a form of \emph{Model Selection}. What was wrong with it?

\end{frame}

\begin{frame}\frametitle{Looking into the Future is Not Legal}

The oos validation is only valid if...

\begin{figure}
\centering
\includegraphics[width=1.2in]{lockbox.png}
\end{figure}

you treat the test set as a lockbox. Once you open it up, that's it! And we opened it up $M$ times!\\~\\ \pause

This is indeed a \emph{Model Selection} procedure but ... \pause our estimate of future generalization error is invalid. How can we do both?
	
\end{frame}

\section{Three Splits and CV}

\begin{frame}\frametitle{In One Fold Let's Focus on the Training Set}
\vspace{-0.2cm}
\begin{figure}
\centering
\includegraphics[width=3.1in]{oos_validation_highlighted}
\end{figure}

\small
\vspace{-0.3cm}
\pause This procedure was completely valid as long as we did not touch the test set, right? \pause As long as we operate only within the training set... we're OK!
	
\end{frame}

\begin{frame}\frametitle{Training $\Rightarrow$ Training \& Validation}
\vspace{-0.5cm}
\begin{figure}
\centering
\includegraphics[width=4.4in]{training_validation}
\end{figure}
	
\end{frame}

\begin{frame}\frametitle{3-way Splitting: The Full Picture}
\vspace{-0.8cm}
\begin{figure}
\centering
\includegraphics[width=4.26in]{3_way_validation}
\end{figure}
	
\end{frame}

\begin{frame}\frametitle{Why Training-Validation-Test Splitting?}

\begin{itemize}
\item Training set: \pause provides fits for many models where overfitting is \qu{okay}
\item Validation set: \pause provides out-of-sample validation for each of the models. If the models are overfit, they will get wrecked at this stage.
\item Test set: \pause this lockbox provides a layer of security against overfitting within the training-validation union set. \pause
\end{itemize}

Just like in the previous 2-way training-test split, you can overfit the training, get killed on the test set and be stuck. How could you similaryl overfit here? \pause Be careful of optimizing to the validation set. Models $1, 2, \ldots, M$ should still be reasonable thought-through models.


\end{frame}


\begin{frame}\frametitle{Sizes of Training-Validation-Test Splits}

Previously, we had the test set being 10-30\%. This recommendation remains. Thus, the training-validation sets together should make up 70-90\% of which a portion is the validation set \pause (which is like a test set) and should be \pause 10-30\% of the total. \pause Thus we arrive at proportions like 50-25-25 or 50-30-20 or 70-20-10. \pause There is no exact guidance here. \\~\\ \pause

The same tradeoffs apply to the test set size but now we have new tradeoffs for the training set size vs. the validation set size:

\begin{itemize}
\item The larger the training set, \pause the better the fit of the model (less bias) but the more variance in its assessment versus its peers
\item The smaller the training set, \pause the worse the fit of the model (more bias) but the less variance in its assessment versus its peers
\end{itemize}
	
\end{frame}

\begin{frame}\frametitle{Back to Wine...}

We can do a single 3-partition split in JMP using the validation column... then data filter... then fit the 6 models again... then use model comparison to select best model ... then undo the filter ... then use model comparison again to find our test set error (the guess of the generalization error)... then build the full model for public consumption.
	
\end{frame}

\begin{frame}\frametitle{Can you CV this 3-Split Procedure?}

\pause

\begin{minipage}{0.6\textwidth}
\begin{figure}
\centering
\includegraphics[width=2.56in]{nested_resampling}
\end{figure}
\end{minipage}~
\begin{minipage}{0.35\textwidth}
\small
Yes. It is called \qu{nested resampling} and $K=10$ fold CV is illustrated here. But... \pause what are you now evaluating? \\~\\

\pause Without CV, it was just model $m^*$. But here $m^*$ varies with the 10 folds! \pause You are testing the entire procedure i.e. given models $1, 2, \ldots, M$, pick the best one and ship it. How well you do in the future is estimated by the CV test set.
\end{minipage}


	
\end{frame}



\begin{frame}\frametitle{Not the First Means for Model Selection}

CV with nested resampling is not generally done in the way it was described here as a means to evaluate a model selection procedure. \pause Beyond scope of course: it is usually done to compare tuning settings in a non-parametric machine learning algorithm. We will see what this means next class. \\~\\ \pause

Neither R nor JMP (to my knowledge) can do this out-of-the-box. In R, even with MLR, you have to program it (MLR uses it for tuning an algorithm). Maybe I will write to them?
	
\end{frame}

\begin{frame}\frametitle{Nested Resampling for Tuning}

\begin{figure}
\centering
\includegraphics[width=3.56in]{nested_resampling_for_tuning.png}
\end{figure}

(from \href{https://mlr-org.github.io/mlr-tutorial/devel/html/nested_resampling/index.html}{MLR's tutorial website}).
\end{frame}

\section{Automatic Model Selection}

\begin{frame}\frametitle{3-way splits for model selection \& evaluation}
\small
Forget CV for a moment since it complicates things ... the \qu{inner split} consisted of the training-validation. We used this to \qu{select} a model based on lowest oos error (RMSE, or highest $R^2$). 

\vspace{-0.2cm}
\begin{figure}
\centering
\includegraphics[width=2.1in]{training_validation}
\end{figure}
\vspace{-0.2cm}
What's the cost? \pause We had to use only 70-90\% of the data (save the test set) to build the model. But what's the bigger cost?? \pause You had to provide models $1, 2, \ldots, M$!! This is no trivial task. So far I've been making them up! I've been using elebaorate linear models which pivoted from param- etric to non-parametric with the addition of curves and interactions. Forget finding the best model... how do we know we even employed good candidates?? \pause \inred{We don't!}

\end{frame}


\begin{frame}\frametitle{Modeling Framework Refresher}
\small
Recall the general regression model:

\beqn
Y = f(x_1, \ldots, x_p) + \errorrv
\eeqn

A couple lectures ago, we made the parametric assumption that: \pause

\beqn
Y = s(x_1, \ldots, x_p; \theta_1, \ldots, \theta_\ell) + \tilde{\errorrv}
\eeqn

where the $\tilde{\errorrv}$ term now includes the previous $\errorrv$ plus $f-s$, the misspecification error. \pause The parametric model $s$ we employed was the linear model and the $\theta$'s we called $\beta$'s: \pause

\beqn
Y = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p + \tilde{\errorrv}
\eeqn

Last lecture, we started adding interactions and polynomials (as well as other transformations e.g. log which we did not cover). \pause This was a means of \qu{expanding} the feature set \qu{visible} to the model using \qu{derived} features: \pause

\beqn
\braces{x_1, \ldots, x_p} \Rightarrow \braces{x'_1, \ldots, x'_{p'}} ~~ \text{where $p' > p$ and maybe much, much greater}
\eeqn

\end{frame}

\begin{frame}\frametitle{\qu{Non-parametric} Linear Regression}

Once we expand this feature set, we can now fit a larger linear model:

\beqn
Y = \beta_0 + \beta_1 x'_1 + \ldots\ldots\ldots\ldots\ldots\ldots\ldots + \beta_p x'_{p'} + \tilde{\errorrv}
\eeqn

Given more degrees of freedom with this expanded feature set allows the linear model to fit more complicated real-world functions. \pause This is essentially a means of doing non-parametric parametric modeling (it's oxymoronic). It's technically parametric but conceptually it's non-parametric since we don't have our parametric benefits: parsimony, inference nor interpretation. Hopefully $\tilde{\errorrv}$ will be close to $\errorrv$, the irreducible noise. \\~\\ \pause

Back to our problem... we can curb overfitting by ... \pause using 3-way split oos validation but we need to select good models... how to do so? \pause One approach is termed \emph{subset selection methods}.
\end{frame}


\begin{frame}\frametitle{Stepwise Regression}

First we expand the feature set from $\braces{x_1, \ldots, x_p} \Rightarrow \braces{x'_1, \ldots, x'_{p'}}$. \pause Then we attempt to find the \qu{best} model consisting of a subset of these features. However there are $2^{p'}$ possible models. For $p'=20$ that's about 1,000,000. \pause So we try to find a model \textit{close} to the optimal using a \qu{heuristic} (a rule of thumb that seems to generally be useful).\\~\\ \pause

That heuristic is called \emph{stepwise} model construction. \pause We begin with \emph{forward stepwise} model construction: \pause

\begin{enumerate}
\item Find the \qu{best} feature from the list of expanded features.\pause
\item Find the \qu{next best} feature from the remaining expanded features.\pause
\item Repeat step 2 until you believe you are overfitting.\pause
\end{enumerate}




\end{frame}

\begin{frame}\frametitle{Estimating Overfitting (again)}
\small
If you choose the feature to give you the best in-sample $R^2$, you will eventually take all the features (until $n = p+1$) and you will get $R^2 = 100\%$. We need a metric to tell us when we may be overfitting and halt at that moment. \pause Here are a few:

\begin{enumerate}
\item oos RMSE (keep a holdout set and quit when this starts increasing) \pause
\item Only include a variable if its $t$ stat (or partial $F$ stat) is significant \pause
\item Use $AICc$.
\end{enumerate}

\beqn
-AIC = 2\loglik{\betahat; \y, \x} - 2p
\eeqn

The first component (the log-likelihood) represents in-sample fit. \pause $\loglik{}$ is like $R^2$ though... as the fit gets closer to the points, the likelihood goes to 1 (and the log likelihood goes to 0). \pause The $2p$ term is a reality check. If you have more features, you are going to overfit. So each additional feature must be justified in terms of the increase in log-likelihood. \pause Thus, good models maximize $-AIC$ (i.e. minimize $AIC$).\\~\\


	

\end{frame}

\begin{frame}\frametitle{$AICc$ for linear models}
\small
For linear regression under OLS, we can calculate the log-likelihood explicitly (we approximately did this in Lecture 2) to obtain:

\beqn
AIC = n\natlog{RMSE^2} + 2p
\eeqn

\pause So once again, we want this to be small. If we decrease $RMSE$ by adding a feature, it needs to counteract an increase of 2 by $p \rightarrow p+1$. If it can't, we're probably overfitting. \pause $AIC$ works well with large sample sizes. For small sample sizes, we use a corrected version $AICc$ defined as:

\beqn
AICc = AIC + \frac{2p(p+1)}{n-p-1}
\eeqn

Needless to say, this is all approximate since we are assuming OLS and a whole bunch of other things (beyond scope of course). \pause Note: there are also BIC and Mallow's $C_p$ which are similar metrics, but we will not cover them. \pause (JMP demo for white wine dataset and telecom set.)


	
\end{frame}

\begin{frame}\frametitle{More About Stepwise Linear Regression}

\small
\emph{Backward selection} begins with all features and then deletes one for each step until no more can justifiably be cut out. \pause Backward selection has a major weakness: it cannot be run on dataframes where the extended feature set is more than the number of rows (only forward or forward with mixed works there). \pause \emph{Mixed selection} begins with either none or all and then looks for both good additions and good subtractions. \\~\\ \pause

Simple case where stepwise doesn't work? \pause How about three features where $x_1$ is most correlated but $x_2$ and $x_3$ together are the best model but there is high collinearity between $x_2$ and $x_3$? What happens? \pause Forward: the model enters $x_1$ and then $x_2$ but it doesn't see $x_3$ as a worthy addition. Backward: the model can nuke $x_2$ or $x_3$ since its p-value or $F$ test is poor. \\~\\ \pause
	
This is not the only way to fit a flexible model and hedge against overfitting. We will do more such models. But first, we will talk about \qu{missing data} as it's more relevant to the project which is due soon.
\end{frame}



\end{document} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}


\begin{frame}\frametitle{}

	
\end{frame}

\begin{frame}\frametitle{}

	
\end{frame}